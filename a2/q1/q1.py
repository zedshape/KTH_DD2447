import numpy as np
import pandas as pd #added manually
import math
import matplotlib.pyplot as plt

def generator(seed, N, M, K, W, alpha_bg, alpha_mw):
    # Data generator.
    # Input: seed: int, N: int, M: int, K: int, W: int, alpha_bg: numpy array with shape(K), alpha_mw: numpy array with shape(K)
    # N = # of sequences
    # M = length of sequences
    # K = alphabet
    # W = length of "magic word"
    # alpha_bg = alpha for background
    # alpha_mw = alpha for magic word
    # Output: D: numpy array with shape (N,M), R_truth: numpy array with shape(N), theta_bg: numpy array with shape (K), theta_mw: numpy array with shape (W,K)


    # D is a set of sequences s1, ..., sn generated by the model
    # the start position is not sampled since it's already known


    np.random.seed(seed)        # Set the seed, initializing pseudorandom number generator

    D = np.zeros((N,M))         # Sequence matrix of size NxM
    theta_bg = np.zeros(K)      # Categorical distribution parameter of background distribution
    theta_mw = np.zeros((W,K))  # Categorical distribution parameter of magic word distribution

    # YOUR CODE:

    # Generate R_truth
    R_samplespace =  M - W + 1
    R_truth = np.random.randint(R_samplespace, size = N)      # integers from discrete uniform distribution

    # generate theta
    for i in range(W):
        theta_mw[i] = np.random.dirichlet(alpha_mw)

    theta_bg = np.random.dirichlet(alpha_bg)

    # Generate D
    for state in range(N):
        for position in range(M):
            #print(" position = " + str(position) + " R_truth = " + str(R_truth[state]))

            if position >= R_truth[state] and position < R_truth[state] + W:
                #print("mw")
                dp = np.nonzero(np.random.multinomial(1,theta_mw[position - R_truth[state]]))[0]
            else:
                #print ("bg")
                dp = np.nonzero(np.random.multinomial(1,theta_bg))[0]

            D[state][position] = dp

    # Generate D, R_truth, theta_bg, theta_mw. Please use the specified data types and dimensions.

    return D, R_truth, theta_bg, theta_mw


def gibbs(D, alpha_bg, alpha_mw, num_iter, W):
    # Gibbs sampler.
    # Input: D: numpy array with shape (N,M),  alpha_bg: numpy array with shape(K), alpha_mw: numpy array with shape(K), num_iter: int, W: int
    # Output: R: numpy array with shape(num_iter, N)

    N = D.shape[0]
    R = np.zeros((num_iter, N)) # Store samples for start positions of magic word of each sequence
    #print("len R " + str(len(R)))

    # YOUR CODE:
    # Implement gibbs sampler for start positions.

    M = D[0].shape[0]
    B = N*(M-W)
    K = alpha_mw.shape[0]

    # randomly setting up initial state r0
    R0 = np.random.randint((M - W + 1), size = N)
    print("R0 = ")
    print(R0)
    #R02 = np.random.randint((M - W + 1), size = N)

    samples = []
    samples.append(R0)      #starting sequence

    for n in range(num_iter):
        positions = []       # this will be a row in R
        current_state=samples[-1]   # the last sampled sequence

        for s in range(N):
            pos_proba = []

            for r_i in range(M - W+1):         # loop over possible start ANDRA EV TILL M_W!
                #current_state = list(current_state)
                current_state[s] = r_i
                count_mw = np.ones((W,K))         # counting occurances of each element at each position in mw
                count_bg = np.ones(K)             # counting occurances of elements in bg

                seq_bg = D
                seq_mw = np.zeros((N,W))

                #for a in range(W):
                #    pass
                    #seq_mw[:,a] = D[:,r_i+a]
                    #seq_bg = np.delete(seq_bg,r_i,axis=1)

                #print(D[n][0:current_state[a]].shape)
                #print(D[n][current_state[a]+W:].shape)
                #print(current_state)

                #seq_bg = [ D[n][0:current_state[a]] + D[n][current_state[a]+W:] for a in range(N)]
                seq_bg = np.asarray([ np.concatenate((D[a][0:current_state[a]], D[a][current_state[a]+W:]), axis = 0) for a in range(N)])
                seq_mw = np.asarray([ D[a][current_state[a]:current_state[a]+W] for a in range(N)])

               # seq_bg = np.delete(seq_bg,s,axis=0)
               #seq_mw = np.delete(seq_mw,s,axis=0)
               # seq_mw = np.delete(seq_mw,1,axis=1)

                for c in range(N):
                    for m in range(M-W):
                    #counts character occurance for char in bg
                        count_bg[int(seq_bg[c][m])] += 1

                    #counts character occurance for every char in every position of mw
                    for w in range(W):
                        count_mw[w][int(seq_mw[c][w])] +=1

                #calculate gamma probabilites
                C = (math.gamma(np.sum(alpha_bg)) / math.gamma(B + np.sum(alpha_bg)))
                class_probs = [ math.gamma(count_bg[k] + alpha_bg[k]) / math.gamma(alpha_bg[k])  for k in range(K)]
                p_bg = math.log(C) + math.log(np.prod(class_probs))
                C2 = math.gamma(np.sum(alpha_mw))/math.gamma(N * W + np.sum(alpha_mw))

                p_mw = []
                for j in range(W):
                    tmp_prob = [ math.gamma(count_mw[j][k] + alpha_mw[k]) / math.gamma(alpha_mw[k])  for k in range(K) ]
                    tmp_prob = math.log(C2) + math.log(np.prod(tmp_prob) )
                    p_mw.append(tmp_prob)

                #print(p_mw)
                #print(class_probs_j)
                p =  p_bg + np.sum(p_mw)
                #print(p)

                pos_proba.append(p)

            #normalize
            p = np.asarray(pos_proba)
            p = np.exp(p - np.max(p))
            p = p/np.sum(p)

            multi_samp = np.random.multinomial(1,p)
            #print(multi_samp)

            #position = np.unravel_index(np.argmax(p), p.shape)[0]
            position = np.argmax(multi_samp)
            current_state[s] = position
            positions.append(position)
            #print(positions)

        samples.append(np.array(positions))

        R[n] = np.array(positions)

    return R

def diagnosis(R):
    #Gelman-Rubin diagnostic
    #reference: http://astrostatistics.psu.edu/RLectures/diagnosticsMCMC.pdf

    n = R.shape[1] #number of iterations
    m = R.shape[0] #number of chains

    #calculate basic thetas
    theta_j = np.mean(R, axis=1)
    theta_bar = np.mean(theta_j, axis=0)

    #calculate W
    sj_square = 1/np.float(n-1)*np.sum(np.array([np.power((R[chain]-theta_j[chain]), 2) \
                                           for chain in range(m)]), axis=1)
    W = 1/np.float(m) * np.sum(sj_square, axis=0)
    #calculate B
    B = n/np.float(m-1)*np.sum(np.power(theta_j-theta_bar, 2), axis=0)

    #Finally get V value
    V_hat = (1-1/n)*W + 1/n*B

    #R
    R_hat = np.sqrt(V_hat/W)

    return R_hat

def main():
    seed = 123

    N = 20 #length of sequence
    M = 10 #length of words
    K = 4
    W = 5 #magic words
    alpha_bg = np.ones(K)
    alpha_mw = np.ones(K) * 0.9 #initial prior
    #alpha_mw = np.array([10, 7, 4, 1])
    num_iter = 1000 #CHANGE BACK TO 1000

    print("Parameters: ", seed, N, M, K, W, num_iter)
    print(alpha_bg)
    print(alpha_mw)

    # Generate synthetic data.
    D, R_truth, theta_bg, theta_mw = generator(seed, N, M, K, W, alpha_bg, alpha_mw)
    print("\nSequences: ")
    print(D)
    print("\nStart positions (truth): ")
    print(R_truth)

    # Use D, alpha_bg and alpha_mw to infer the start positions of magic words.
    R = gibbs(D, alpha_bg, alpha_mw, num_iter, W)
    print("\nStart positions (sampled): ")
    print(R[0,:])
    print(R[1,:])


    # YOUR CODE:

    #####plotting area
    #We commented in all plt.show() for submission
    #Grader can comment them out to check our result

    print("Now the code will calculate things to plot, but we commented in all plt.show() part")
    #1. prediction hit rate
    f, ax = plt.subplots(20, 1, figsize=(16, 50), dpi=80)
    R = np.array(R)
    for n in range(N):
        ax[n].plot(R[:,n])
        ax[n].set_title('Position $m={}$'.format(n))
    print("We calculated prediction trace")
    #plt.show()

    #2. histogram plottnig

    f, ax = plt.subplots(2, 10, figsize=(30, 10), dpi=80)
    for n in range(N):
        ax[(n//10), n%10].hist(R[:, n], bins=100)
        ax[n//10, n%10].set_title('Position {}'.format(n), fontsize=20)
    print("We calculated histogram")
    #plt.show()

    #hit rate
    truth = np.array(R_truth)
    truth
    hit = []
    for n in range(N):
        hit.append(float(np.sum(R[:, n] == truth[n])/len(R[:,n])))
    print("We calculated hit rate")
    print(hit)

    #3. convergence check
    #we commented whole part as it takes a lot of time to get 10 multiple_models
    """
    multiple_gibbs = [gibbs(D, alpha_bg, alpha_mw, num_iter, W) for _ in range(10)]
    multiple_gibbs = np.array(multiple_gibbs)
    print("now we run 10 models to check convergence rate")
    diagnosis_results = np.array([ diagnosis(multiple_gibbs[:,0:t,:]) for t in range(2,num_iter)])
    plt.figure(figsize=(16,6))
    plt.title('Diagnosis result(Convergence rate) using Gelman-Rubin diagnostic')
    for n in range(N): # for each position plot a new graph
        plt.plot(diagnosis_results[:,n], label=n)
    plt.legend(loc=1)
    print("We calculated convergence rate")
    plt.show()
    """

if __name__ == '__main__':  #autoruns main
    main()
